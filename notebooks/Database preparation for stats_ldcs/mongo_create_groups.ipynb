{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "# from sources import *\n",
    "import os\n",
    "import os.path\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from geo.countries import Countries\n",
    "from geo.utils import extract_countries\n",
    "from geo.data import country_alias\n",
    "c = Countries()\n",
    "c.load_wb()\n",
    "from data.Frame import Frame\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "import requests\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import db\n",
    "\n",
    "# Get all json files containing group definitions\n",
    "def get_group_json_files():\n",
    "    from pathlib import Path\n",
    "    path = Path(os.getcwd())\n",
    "    files = []\n",
    "    for f in os.listdir(str(path.parent.parent) + '//geo///cache/groups//'):\n",
    "        files.append(str(path.parent.parent) + '//geo//cache//groups//' + f)\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Do some cleaning and adjustments to make names uniform\n",
    "def replace(file, name, aliases):\n",
    "    for alias in aliases:\n",
    "        with open(file, 'r', encoding='utf8') as f:\n",
    "            data = f.read()\n",
    "            se = (\"%s%s%s\" %(\"'\", alias,\"'\"))\n",
    "            data = data.replace(se, name)\n",
    "            with open(file, 'w', encoding='utf8') as f:\n",
    "                f.write(data)\n",
    "                f.close()\n",
    "                \n",
    "for country in country_alias:\n",
    "    for gfile in get_group_json_files():\n",
    "        replace(gfile, country, country_alias[country])\n",
    "\n",
    "# Read json file\n",
    "def read_json_file(fi):\n",
    "    with open(fi, encoding='utf-8') as f:\n",
    "        d = json.load(f)\n",
    "    return d\n",
    "\n",
    "# Load country data from restcountries\n",
    "def read_restcountries():\n",
    "    url = 'https://restcountries.eu/rest/v2/all'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    country_names = json.loads(data)\n",
    "    return country_names\n",
    "\n",
    "rest_countries = read_restcountries()\n",
    "\n",
    "# Searc rest countries\n",
    "def get_country(name):\n",
    "    for c in rest_countries:\n",
    "        if c['name'] == name:\n",
    "            return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//acd.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//acp.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//ag.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//aosis.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//ap.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//asean.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//au.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//brics.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//can.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//canz.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//caricom.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//cegpl.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//cplp.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//cw.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//eac.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//ecowas.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//eu.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//g20.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//g24.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//g77.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//gcc.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//grulac.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//hics.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//ida.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//ioc.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//las.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//ldcs.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//lics.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//lldcs.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//lmcs.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//mercosur.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//nam.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//nato.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//nordic.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//oas.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//oecd.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//oic.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//osce.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//pif.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//sadc.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//sica.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//sids.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//umcs.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//un.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//weog.json\n",
      "Processing C:\\Users\\tomas\\datascrapper2//geo//cache//groups//zangger.json\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient('stats.whereistomas.org', username='root', password='Root!0676')\n",
    "db = client[\"stats\"]\n",
    "url = 'http://country.io/names.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read().decode('utf-8')\n",
    "country_names = json.loads(data)\n",
    "\n",
    "db[\"groups\"].drop()\n",
    "\n",
    "for f in get_group_json_files():    \n",
    "    print (\"Processing %s\" % (f))\n",
    "    if \"json\" in f:\n",
    "        data = read_json_file(f)\n",
    "        countries = [] \n",
    "        # Add additional country information\n",
    "        for n in data['names']:\n",
    "            countries.append(get_country(n))\n",
    "            \n",
    "        data['countries'] = countries\n",
    "        \n",
    "        db[\"groups\"].insert_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascrapper2",
   "language": "python",
   "name": "datascrapper2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
